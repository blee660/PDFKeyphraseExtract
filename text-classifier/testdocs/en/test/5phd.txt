Information occlusion on small displays is a familiar problem affecting interaction on multiple levels. This research investigates solutions to two causes of screen occlusion: first, temporal and spatial occlusion caused by magnification strategies such as zooming and overview + detail interfaces and, second, the physical occlusion caused by fingers interacting with the display. The findings from each area considerably reduce screen occlusion on small displays.To overcome occlusions caused by magnification interfaces, distortion-based strategies are used and applied to the sketching domain for rapid content creation and editing. The use of distortion-based magnification strategies has been limited to viewing and selecting information; usability problems, such as the comprehension difficulties caused by the visual transformations employed, have not been solved. A novel combination of sketching is used as the context to investigate information creation and editing using distortion techniques. Sketching allows to identify the requirements for content creation and editing, and also addresses problems shared with content viewing and selecting. To investigate the requirements for magnified sketching, first the problems existing distortion interfaces have when used for sketching are identified. Next, solutions to address the identified problems are developed and tested iteratively before concluding with a comparison of the improved distortion interface and other magnification strategies. The results of this final comparison of the distortion, zooming and overview + detail interfaces demonstrate the improved distortion interface’s efficacy and that it is the participants’ preferred interface for magnified content editing and creation.To overcome finger-based occlusion, the use of 3D gestures captured by a camera is investigated, thus moving the interaction space away from the display. When entering data on a touchscreen using the fingers, the information below the finger is covered and thus hidden from the user. An input mechanism is developed and evaluated which uses 3D gestures captured by the front-facing camera. As this input metaphor is novel, the first step was to investigate the requirements of such an interaction on the device as well as on the user. To this end, a set of gestures of varying demands on the user was devised and recognition algorithms for these gestures developed. The resultant gesture interface was tested on users. The results show the potential of the new interaction mechanism and provide further insight into successful and robust recognition strategies.